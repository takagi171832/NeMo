{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-09 02:16:11 nemo_logging:349] /home/takagi/NeMo/nemo_env_v2/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "      warnings.warn(\"Can't initialize NVML\")\n",
      "    \n",
      "[NeMo W 2024-02-09 02:16:11 nemo_logging:349] /home/takagi/NeMo/nemo_env_v2/lib/python3.10/site-packages/torch/cuda/__init__.py:740: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "      return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-09 02:16:12 mixins:170] Tokenizer SentencePieceTokenizer initialized with 4050 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-09 02:16:13 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /home/takagi/NeMo/manifests/CSJ/train_nodup_sp/train_nodup_sp_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2024-02-09 02:16:13 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /home/takagi/NeMo/manifests/CSJ/train_dev/train_dev_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-02-09 02:16:13 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /home/takagi/NeMo/manifests/CSJ/eval1/eval1_manifest.json\n",
      "    - /home/takagi/NeMo/manifests/CSJ/eval2/eval2_manifest.json\n",
      "    - /home/takagi/NeMo/manifests/CSJ/eval3/eval3_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-09 02:16:13 features:289] PADDING: 0\n",
      "[NeMo I 2024-02-09 02:16:15 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/takagi/NeMo/models/ASR/uni_LSTM_8_bpe_CSJ.nemo.\n"
     ]
    }
   ],
   "source": [
    "model = nemo_asr.models.EncDecCTCModel.restore_from(\"/home/takagi/NeMo/models/ASR/uni_LSTM_8_bpe_CSJ.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): RNNEncoder(\n",
       "    (pre_encode): StackingSubsampling(\n",
       "      (proj_out): Linear(in_features=320, out_features=640, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (4): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (7): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "      (9): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (10): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (11): Dropout(p=0.2, inplace=False)\n",
       "      (12): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (13): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (14): Dropout(p=0.2, inplace=False)\n",
       "      (15): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (16): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (17): Dropout(p=0.2, inplace=False)\n",
       "      (18): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (19): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (20): Dropout(p=0.2, inplace=False)\n",
       "      (21): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (22): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (23): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(640, 4051, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (_wer): WERBPE()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"/home/takagi/espnet/egs2/csj/asr1/dump/raw/eval1/data/format.1/A01M0097_0000211_0001695.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): RNNEncoder(\n",
       "    (pre_encode): StackingSubsampling(\n",
       "      (proj_out): Linear(in_features=320, out_features=640, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (4): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (7): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "      (9): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (10): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (11): Dropout(p=0.2, inplace=False)\n",
       "      (12): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (13): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (14): Dropout(p=0.2, inplace=False)\n",
       "      (15): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (16): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (17): Dropout(p=0.2, inplace=False)\n",
       "      (18): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (19): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (20): Dropout(p=0.2, inplace=False)\n",
       "      (21): LSTM(640, 1024, proj_size=640, batch_first=True)\n",
       "      (22): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (23): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(640, 4051, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (_wer): WERBPE()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_file_path, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.484"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio) / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23744"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23744,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo_env_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
